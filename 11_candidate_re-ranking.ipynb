{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a106dd4",
   "metadata": {},
   "source": [
    "## Step 0 — Install (once) & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87fbc224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not installed already (safe to rerun)\n",
    "# %pip install lightgbm==4.3.0 joblib==1.4.2\n",
    "\n",
    "import os, json, math, gc, random\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9877d117",
   "metadata": {},
   "source": [
    "### Step 1 — Config and knobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fea3c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N_PER_ANCHOR = 40\n",
    "FALLBACK_MIN_CANDS = 30\n",
    "NEG_PER_CART = 20\n",
    "MAX_TRAIN_BASKETS = 80_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3bd6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FILENAMES (same folder) ===\n",
    "FILE_MODELING = \"modeling_dataset.csv\"  # your full ~1.4M modeling dataset (if named different, change)\n",
    "FILE_TEST     = \"test_data_question.csv\"\n",
    "FILE_P        = \"P_j_given_i.csv\"\n",
    "FILE_LIFT     = \"item_lift_matrix.csv\"\n",
    "FILE_JACC     = \"item_jaccard_matrix.csv\"\n",
    "FILE_COOC     = \"item_cooccurrence_counts.csv\"\n",
    "FILE_POP      = \"item_stats_counts_and_freq.csv\"\n",
    "\n",
    "# === COLUMN MAPPINGS known from your data ===\n",
    "POPULAR_COL_ITEM = \"Unnamed: 0\"  # item name column in item_stats_counts_and_freq.csv\n",
    "POPULAR_COL_FREQ = \"freq\"\n",
    "\n",
    "# P_j_given_i: we will auto-melt wide to long if needed\n",
    "COL_ANCHOR = \"anchor_item\"\n",
    "COL_CAND   = \"candidate_item\"\n",
    "COL_PROB   = \"P_j_given_i\"\n",
    "\n",
    "# === Stage-1 candidate gen knobs used for both training & inference ===\n",
    "TOP_N_PER_ANCHOR = 60         # how many candidates per anchor item (before aggregation)\n",
    "FALLBACK_MIN_CANDS = 40       # ensure at least this many candidates per cart\n",
    "AGG_PROB_METHOD = \"one_minus_prod\"  # {\"one_minus_prod\",\"sum\",\"max\"}\n",
    "AGG_OTHER_METHOD = \"mean\"           # {\"mean\",\"sum\",\"max\"}\n",
    "\n",
    "W_PROB = 0.60\n",
    "W_LIFT = 0.25\n",
    "W_JACC = 0.10\n",
    "W_POPU = 0.05\n",
    "\n",
    "# === Training set size (speed/scale) ===\n",
    "MAX_TRAIN_BASKETS = 150_000   # cap number of LOO carts from modeling (adjust per machine)\n",
    "NEG_PER_CART      = 40        # sampled negatives per cart (downsample to keep train small)\n",
    "RANDOM_SEED       = 42\n",
    "N_JOBS            = max(1, os.cpu_count() - 1)  # parallel jobs for feature gen\n",
    "\n",
    "# === Validation split ===\n",
    "# Use time-based split (last X% of orders for validation)\n",
    "VAL_FRACTION_TIME = 0.10  # use last 10% by date as validation\n",
    "\n",
    "# === Output files ===\n",
    "FILE_STAGE2_MODEL = \"stage2_lgbm_model.txt\"              # saved LightGBM model\n",
    "FILE_STAGE2_FEATS = \"stage2_feature_columns.json\"        # persisted feature list\n",
    "FILE_STAGE2_SUB   = \"stage2_recommendations_top3.csv\"    # final submission csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209870fe",
   "metadata": {},
   "source": [
    "### Step 2 — Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "212d08d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize_item(x):\n",
    "    if pd.isna(x): return x\n",
    "    return str(x).strip()\n",
    "\n",
    "def safe_minmax(s: pd.Series):\n",
    "    s = s.astype(float).fillna(0.0)\n",
    "    lo, hi = s.min(), s.max()\n",
    "    if hi - lo < 1e-12:\n",
    "        return pd.Series(np.zeros(len(s), dtype=float), index=s.index)\n",
    "    return (s - lo) / (hi - lo)\n",
    "\n",
    "def one_minus_product_of_complements(values):\n",
    "    v = np.clip(np.array(values, dtype=float), 0.0, 1.0)\n",
    "    return float(1.0 - np.prod(1.0 - v))\n",
    "\n",
    "def item_bucket(name: str) -> str:\n",
    "    n = str(name).lower()\n",
    "    if \"fries\" in n: return \"fries\"\n",
    "    if \"combo\" in n: return \"combo\"\n",
    "    if \"dip\" in n: return \"dip\"\n",
    "    if \"corn\" in n: return \"sides\"\n",
    "    if any(w in n for w in [\"cake\",\"brownie\",\"cookie\"]): return \"dessert\"\n",
    "    if \"drink\" in n or \"soda\" in n: return \"drink\"\n",
    "    if \"wings\" in n and \"spicy\" in n: return \"wings_spicy\"\n",
    "    if \"wings\" in n and \"grilled\" in n: return \"wings_grilled\"\n",
    "    if \"wings\" in n: return \"wings\"\n",
    "    if \"strips\" in n: return \"strips\"\n",
    "    return \"other\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a170d5",
   "metadata": {},
   "source": [
    "### Step 3 — Load tables and normalize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23ec10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Popularity\n",
    "pop_df = pd.read_csv(FILE_POP)\n",
    "pop_df = pop_df[[POPULAR_COL_ITEM, POPULAR_COL_FREQ]].dropna()\n",
    "pop_df[POPULAR_COL_ITEM] = pop_df[POPULAR_COL_ITEM].map(canonicalize_item)\n",
    "pop_df = pop_df.sort_values(POPULAR_COL_FREQ, ascending=False).reset_index(drop=True)\n",
    "\n",
    "# 3.2 P(j|i) table\n",
    "P_raw = pd.read_csv(FILE_P)\n",
    "# If it's a wide N x N matrix, melt it:\n",
    "if COL_PROB not in P_raw.columns or len({'item','Item','ITEM'}.intersection(P_raw.columns))>0:\n",
    "    row_item_col = 'item' if 'item' in P_raw.columns else ('Item' if 'Item' in P_raw.columns else P_raw.columns[0])\n",
    "    P = P_raw.melt(id_vars=[row_item_col], var_name=COL_CAND, value_name=COL_PROB)\\\n",
    "             .rename(columns={row_item_col: COL_ANCHOR})\n",
    "else:\n",
    "    P = P_raw.rename(columns={P_raw.columns[0]: COL_ANCHOR,\n",
    "                              P_raw.columns[1]: COL_CAND,\n",
    "                              P_raw.columns[2]: COL_PROB})\n",
    "P[COL_ANCHOR] = P[COL_ANCHOR].map(canonicalize_item)\n",
    "P[COL_CAND]   = P[COL_CAND].map(canonicalize_item)\n",
    "P = P.dropna(subset=[COL_CAND, COL_PROB])\n",
    "\n",
    "# 3.3 Lift / Jaccard / Co-occurrence: melt to long if needed\n",
    "def to_long_pairs(df, value_col):\n",
    "    cols = df.columns.tolist()\n",
    "    str_cols = [c for c in cols if df[c].dtype == \"object\"]\n",
    "    if value_col in cols and len(str_cols) >= 2:\n",
    "        # already long\n",
    "        return df.rename(columns={str_cols[0]: COL_ANCHOR, str_cols[1]: COL_CAND})\n",
    "    base = df.copy()\n",
    "    row_item_col = cols[0]\n",
    "    base[row_item_col] = base[row_item_col].map(canonicalize_item)\n",
    "    long_df = base.melt(id_vars=[row_item_col], var_name=COL_CAND, value_name=value_col)\\\n",
    "                  .rename(columns={row_item_col: COL_ANCHOR})\n",
    "    long_df[COL_CAND] = long_df[COL_CAND].map(canonicalize_item)\n",
    "    return long_df\n",
    "\n",
    "lift_pairs = to_long_pairs(pd.read_csv(FILE_LIFT),  \"lift\")\n",
    "jacc_pairs = to_long_pairs(pd.read_csv(FILE_JACC),  \"jaccard\")\n",
    "cooc_pairs = to_long_pairs(pd.read_csv(FILE_COOC),  \"cooc_count\")\n",
    "\n",
    "# Keep only needed columns & drop NaNs\n",
    "lift_pairs = lift_pairs[[COL_ANCHOR, COL_CAND, \"lift\"]].dropna()\n",
    "jacc_pairs = jacc_pairs[[COL_ANCHOR, COL_CAND, \"jaccard\"]].dropna()\n",
    "cooc_pairs = cooc_pairs[[COL_ANCHOR, COL_CAND, \"cooc_count\"]].dropna()\n",
    "\n",
    "# 3.4 Build fast per-anchor top-N\n",
    "P_sorted = P.sort_values([COL_ANCHOR, COL_PROB], ascending=[True, False]).copy()\n",
    "P_topN = P_sorted.groupby(COL_ANCHOR, as_index=False).head(TOP_N_PER_ANCHOR)\n",
    "P_by_anchor = {a: g[[COL_CAND, COL_PROB]].reset_index(drop=True)\n",
    "               for a, g in P_topN.groupby(COL_ANCHOR)}\n",
    "\n",
    "POPULAR_ITEMS = pop_df[POPULAR_COL_ITEM].tolist()\n",
    "\n",
    "# === Step 3.5 — Prejoin signals once per anchor to avoid merging inside every call ===\n",
    "pairs_all = P_topN.merge(lift_pairs, on=[COL_ANCHOR, COL_CAND], how=\"left\") \\\n",
    "                  .merge(jacc_pairs, on=[COL_ANCHOR, COL_CAND], how=\"left\") \\\n",
    "                  .merge(cooc_pairs, on=[COL_ANCHOR, COL_CAND], how=\"left\")\n",
    "\n",
    "# Build a dict: anchor -> DataFrame of [candidate, prob, lift, jaccard, cooc_count]\n",
    "CAND_BY_ANCHOR = {a: g[[COL_CAND, COL_PROB, \"lift\", \"jaccard\", \"cooc_count\"]].reset_index(drop=True)\n",
    "                  for a, g in pairs_all.groupby(COL_ANCHOR)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ab0c3e",
   "metadata": {},
   "source": [
    "### Step 4 — Detect menu-item columns in the modeling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c7f8a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only load header to detect columns fast\n",
    "modeling_header = pd.read_csv(FILE_MODELING, nrows=5)\n",
    "menu_items = set(pop_df[POPULAR_COL_ITEM].unique())\n",
    "ITEM_COLS = [c for c in modeling_header.columns if c in menu_items]\n",
    "\n",
    "ID_COLS = [\n",
    "    \"CUSTOMER_ID\",\"STORE_NUMBER\",\"ORDER_CREATED_DATE\",\"ORDER_ID\",\n",
    "    \"ORDER_CHANNEL_NAME\",\"ORDER_SUBCHANNEL_NAME\",\"ORDER_OCCASION_NAME\"\n",
    "]\n",
    "\n",
    "CUST_CTX_COLS = [\n",
    "    # customer-type one-hots (based on your schema)\n",
    "    \"cust_registered\",\"cust_guest\",\"cust_special_membership\",\n",
    "    # customer features\n",
    "    \"orders_count\",\"items_count\",\"repeat_purchase_rate\",\"avg_order_value\",\n",
    "    # preferred/favorite\n",
    "    \"favorite_item\",\n",
    "    # contextual features baked in\n",
    "    \"weekend_order_ratio\",\"most_common_order_hour\",\"most_common_order_dow\",\n",
    "    \"most_common_store\",\"store_diversity_count\"\n",
    "]\n",
    "\n",
    "STORE_ONEHOTS = [c for c in modeling_header.columns if c.startswith(\"store_city_\")] + ([\"store_STATE\"] if \"store_STATE\" in modeling_header.columns else [])\n",
    "\n",
    "# Keep only columns we’ll actually use to avoid memory blow-ups\n",
    "KEEP_COLS = ID_COLS + ITEM_COLS + CUST_CTX_COLS + STORE_ONEHOTS + ([\"total_order_price\"] if \"total_order_price\" in modeling_header.columns else [])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035828ca",
   "metadata": {},
   "source": [
    "### Step 5 — Stage-1 candidate generator (vectorized, same logic as earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8da2f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates_for_cart(cart_items):\n",
    "    # 1) Collect per-anchor frames fast (no merges here)\n",
    "    frames = []\n",
    "    for anchor in cart_items:\n",
    "        df = CAND_BY_ANCHOR.get(anchor)\n",
    "        if df is not None and not df.empty:\n",
    "            tmp = df.copy()\n",
    "            tmp[\"_anchor\"] = anchor\n",
    "            frames.append(tmp)\n",
    "\n",
    "    if not frames:\n",
    "        # popularity fallback\n",
    "        fb = pop_df[~pop_df[POPULAR_COL_ITEM].isin(cart_items)] \\\n",
    "             .rename(columns={POPULAR_COL_ITEM: COL_CAND}).copy()\n",
    "        fb[\"stage1_score\"] = safe_minmax(fb[POPULAR_COL_FREQ])\n",
    "        fb[\"votes\"] = 0\n",
    "        return fb[[COL_CAND, \"stage1_score\", \"votes\"]].head(FALLBACK_MIN_CANDS)\n",
    "\n",
    "    cand_pairs = pd.concat(frames, ignore_index=True)\n",
    "    cand_pairs = cand_pairs[~cand_pairs[COL_CAND].isin(cart_items)].copy()\n",
    "\n",
    "    # 2) Aggregate across anchors\n",
    "    def agg_prob(s):\n",
    "        if AGG_PROB_METHOD == \"one_minus_prod\": return one_minus_product_of_complements(s)\n",
    "        if AGG_PROB_METHOD == \"sum\": return s.sum()\n",
    "        return s.max()\n",
    "    def agg_other(s):\n",
    "        if AGG_OTHER_METHOD == \"mean\": return s.mean(skipna=True)\n",
    "        if AGG_OTHER_METHOD == \"sum\": return s.sum(skipna=True)\n",
    "        return s.max(skipna=True)\n",
    "\n",
    "    grouped = cand_pairs.groupby(COL_CAND).agg({\n",
    "        COL_PROB: agg_prob,\n",
    "        \"lift\": agg_other,\n",
    "        \"jaccard\": agg_other,\n",
    "        \"cooc_count\": \"sum\",\n",
    "        \"_anchor\": \"count\"     # <-- votes (how many anchors suggested this candidate)\n",
    "    }).rename(columns={\"_anchor\": \"votes\"}).reset_index()\n",
    "\n",
    "    # 3) Attach popularity & score\n",
    "    grouped = grouped.merge(\n",
    "        pop_df.rename(columns={POPULAR_COL_ITEM: COL_CAND, POPULAR_COL_FREQ: \"popularity\"}),\n",
    "        on=COL_CAND, how=\"left\"\n",
    "    )\n",
    "\n",
    "    for col in [COL_PROB, \"lift\", \"jaccard\", \"popularity\"]:\n",
    "        if col not in grouped.columns: grouped[col] = 0.0\n",
    "        grouped[f\"{col}_norm\"] = safe_minmax(grouped[col])\n",
    "\n",
    "    grouped[\"stage1_score\"] = (\n",
    "        W_PROB*grouped[f\"{COL_PROB}_norm\"] +\n",
    "        W_LIFT*grouped[\"lift_norm\"] +\n",
    "        W_JACC*grouped[\"jaccard_norm\"] +\n",
    "        W_POPU*grouped[\"popularity_norm\"]\n",
    "    )\n",
    "\n",
    "    # 4) Ensure minimum candidates\n",
    "    need = max(0, FALLBACK_MIN_CANDS - len(grouped))\n",
    "    if need > 0:\n",
    "        fb = pop_df[~pop_df[POPULAR_COL_ITEM].isin(set(grouped[COL_CAND]).union(cart_items))] \\\n",
    "                  .head(need).rename(columns={POPULAR_COL_ITEM: COL_CAND})\n",
    "        fb[\"stage1_score\"] = 0.0\n",
    "        fb[\"votes\"] = 0\n",
    "        grouped = pd.concat([grouped[[COL_CAND,\"stage1_score\",\"votes\",COL_PROB,\"lift\",\"jaccard\",\"popularity\"]],\n",
    "                             fb[[COL_CAND,\"stage1_score\",\"votes\"]]],\n",
    "                            ignore_index=True)\n",
    "\n",
    "    return grouped.sort_values(\"stage1_score\", ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f399fc5",
   "metadata": {},
   "source": [
    "### Step 6 — Create Leave-One-Out (LOO) training pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fc5f24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train pairs: (3637100, 22)\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "def row_to_cart_items(row_items):\n",
    "    # return list of item cols present (value > 0)\n",
    "    return [c for c, v in row_items.items() if float(v) > 0]\n",
    "\n",
    "def build_pairs_for_row(row):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with columns:\n",
    "    - 'cart_id', 'candidate_item', 'label', 'ORDER_CREATED_DATE' (+ features in next step)\n",
    "    cart_id is ORDER_ID to keep a unique group per cart in training.\n",
    "    \"\"\"\n",
    "    # basket items from one-hot/counts\n",
    "    items = [c for c in ITEM_COLS if float(row.get(c, 0)) > 0]\n",
    "    if len(items) < 2:\n",
    "        return None\n",
    "\n",
    "    heldout = rng.choice(items)\n",
    "    cart = [it for it in items if it != heldout]\n",
    "\n",
    "    # Stage-1 candidates (vectorized aggregation)\n",
    "    ranked = get_candidates_for_cart(cart)\n",
    "\n",
    "    # --- NEW: canonicalize both sides to avoid string mismatches ---\n",
    "    heldout = canonicalize_item(heldout)\n",
    "    ranked[COL_CAND] = ranked[COL_CAND].map(canonicalize_item)\n",
    "\n",
    "    # Ensure the true item is present (pre-truncation)\n",
    "    if heldout not in set(ranked[COL_CAND]):\n",
    "        ranked = pd.concat(\n",
    "            [pd.DataFrame({COL_CAND:[heldout],\n",
    "                        \"stage1_score\":[(ranked[\"stage1_score\"].min() if len(ranked) else 0) - 1e-6],\n",
    "                        COL_PROB:[0.0], \"lift\":[0.0], \"jaccard\":[0.0], \"popularity\":[0.0]}),\n",
    "            ranked],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "\n",
    "    # Sample negatives to keep per-cart small\n",
    "    target = max(NEG_PER_CART+1, 50)\n",
    "    ranked = ranked.head(target)\n",
    "\n",
    "    # --- NEW: ensure the positive survived truncation ---\n",
    "    if not (ranked[COL_CAND] == heldout).any():\n",
    "        extra = pd.DataFrame({COL_CAND:[heldout],\n",
    "                            \"stage1_score\":[ranked[\"stage1_score\"].min() - 1e-6],\n",
    "                            COL_PROB:[0.0], \"lift\":[0.0], \"jaccard\":[0.0], \"popularity\":[0.0]})\n",
    "        # put it at the top and re-trim to keep length == target\n",
    "        ranked = pd.concat([extra, ranked], ignore_index=True).head(target)\n",
    "\n",
    "    # Labels\n",
    "    ranked[\"label\"] = (ranked[COL_CAND] == heldout).astype(int)\n",
    "\n",
    "    out = ranked.copy()\n",
    "    out[\"cart_id\"] = row[\"ORDER_ID\"]  # group id\n",
    "    out[\"ORDER_CREATED_DATE\"] = row.get(\"ORDER_CREATED_DATE\")\n",
    "    # Keep some cart-level descriptors (optional for features)\n",
    "    out[\"cart_size\"] = len(cart)\n",
    "    out[\"anchors_voted\"] = out.get(\"votes\", 0)\n",
    "    out[\"cart_has_combo\"] = int(any(\"combo\" in it.lower() for it in cart))\n",
    "    out[\"cart_has_wings\"] = int(any(\"wings\" in it.lower() for it in cart))\n",
    "    out[\"cart_has_fries\"] = int(any(\"fries\" in it.lower() for it in cart))\n",
    "    out[\"cart_has_dip\"]   = int(any(\"dip\"   in it.lower() for it in cart))\n",
    "    out[\"candidate_bucket\"] = out[COL_CAND].map(item_bucket)\n",
    "    return out\n",
    "\n",
    "def iterate_modeling_rows():\n",
    "    # chunked read for memory efficiency\n",
    "    usecols = [c for c in KEEP_COLS if c in modeling_header.columns]\n",
    "    for chunk in pd.read_csv(FILE_MODELING, usecols=usecols, chunksize=50_000):\n",
    "        # ensure proper dtypes\n",
    "        for c in ITEM_COLS:\n",
    "            if c in chunk.columns:\n",
    "                chunk[c] = chunk[c].fillna(0).astype(float)\n",
    "        yield chunk\n",
    "\n",
    "# Build train pool (sample up to MAX_TRAIN_BASKETS rows that have >=2 items)\n",
    "train_pairs = []\n",
    "seen = 0\n",
    "\n",
    "for chunk in iterate_modeling_rows():\n",
    "    # Keep baskets with >=2 items\n",
    "    basket_mask = chunk[ITEM_COLS].sum(axis=1) >= 2\n",
    "    cand_chunk = chunk.loc[basket_mask].copy()\n",
    "\n",
    "    # --- NEW: ensure one LOO per ORDER_ID in this pass ---\n",
    "    cand_chunk = cand_chunk.drop_duplicates(subset=[\"ORDER_ID\"])\n",
    "\n",
    "    # time split prep\n",
    "    # parse date once\n",
    "    cand_chunk[\"ORDER_CREATED_DATE\"] = pd.to_datetime(cand_chunk[\"ORDER_CREATED_DATE\"])\n",
    "\n",
    "    # sample subset to control size\n",
    "    if len(cand_chunk) > 0:\n",
    "        needed = max(0, MAX_TRAIN_BASKETS - seen)\n",
    "        if needed <= 0:\n",
    "            break\n",
    "        frac = min(1.0, needed / len(cand_chunk))\n",
    "        sampled = cand_chunk.sample(frac=frac, random_state=RANDOM_SEED)\n",
    "        seen += len(sampled)\n",
    "\n",
    "        # Parallel per-row pair building\n",
    "        dict_rows = sampled.to_dict(orient=\"records\")\n",
    "        # results = Parallel(n_jobs=N_JOBS, backend=\"loky\", verbose=0)(\n",
    "        #     delayed(build_pairs_for_row)(row) for row in dict_rows\n",
    "        # )\n",
    "        results = Parallel(\n",
    "            n_jobs=N_JOBS,\n",
    "            backend=\"threading\",        # <-- was \"loky\"\n",
    "            batch_size=256,             # <-- add batching to reduce scheduling overhead\n",
    "            verbose=0\n",
    "        )(delayed(build_pairs_for_row)(row) for row in dict_rows)\n",
    "        \n",
    "        results = [r for r in results if r is not None]\n",
    "        if results:\n",
    "            train_pairs.append(pd.concat(results, ignore_index=True))\n",
    "\n",
    "    # free memory\n",
    "    del cand_chunk, chunk\n",
    "    gc.collect()\n",
    "\n",
    "train_df = pd.concat(train_pairs, ignore_index=True) if train_pairs else pd.DataFrame()\n",
    "print(\"Train pairs:\", train_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e891e1a3",
   "metadata": {},
   "source": [
    "### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44d4d8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows=3,637,100  carts=72,742  avg_per_cart=50.0\n",
      "positive rate (should be ~1/(NEG_PER_CART+1)): 0.0200\n",
      "carts with NO positive (should be 0): 0\n",
      "cart size stats:\n",
      " count    72742.0\n",
      "mean        50.0\n",
      "std          0.0\n",
      "min         50.0\n",
      "25%         50.0\n",
      "50%         50.0\n",
      "75%         50.0\n",
      "max         50.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "n_rows = len(train_df)\n",
    "n_carts = train_df['cart_id'].nunique()\n",
    "avg_per_cart = n_rows / max(1, n_carts)\n",
    "pos_rate = train_df['label'].mean()\n",
    "cart_without_pos = (train_df.groupby('cart_id')['label'].max() == 0).sum()\n",
    "cart_size_stats = train_df.groupby('cart_id').size().describe()\n",
    "\n",
    "print(f\"rows={n_rows:,}  carts={n_carts:,}  avg_per_cart={avg_per_cart:.1f}\")\n",
    "print(f\"positive rate (should be ~1/(NEG_PER_CART+1)): {pos_rate:.4f}\")\n",
    "print(f\"carts with NO positive (should be 0): {cart_without_pos}\")\n",
    "print(\"cart size stats:\\n\", cart_size_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be9e3ac",
   "metadata": {},
   "source": [
    "### Step 7 — Feature engineering for pairs (cart, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1135b454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Candidate attributes\n",
    "train_df = train_df.merge(pop_df.rename(columns={POPULAR_COL_ITEM: COL_CAND,\n",
    "                                                 POPULAR_COL_FREQ: \"cand_popularity\"}),\n",
    "                          on=COL_CAND, how=\"left\")\n",
    "\n",
    "# Stage-1 ranks\n",
    "train_df[\"stage1_rank\"] = train_df.groupby(\"cart_id\")[\"stage1_score\"].rank(ascending=False, method=\"first\")\n",
    "\n",
    "# Candidate type/bucket one-hot (small set)\n",
    "train_df[\"cand_is_combo\"]   = (train_df[\"candidate_bucket\"] == \"combo\").astype(int)\n",
    "train_df[\"cand_is_fries\"]   = (train_df[\"candidate_bucket\"] == \"fries\").astype(int)\n",
    "train_df[\"cand_is_dip\"]     = (train_df[\"candidate_bucket\"] == \"dip\").astype(int)\n",
    "train_df[\"cand_is_wings\"]   = train_df[\"candidate_bucket\"].isin([\"wings\",\"wings_spicy\",\"wings_grilled\"]).astype(int)\n",
    "train_df[\"cand_is_strips\"]  = (train_df[\"candidate_bucket\"] == \"strips\").astype(int)\n",
    "\n",
    "# Bring customer/context columns from modeling set (for each cart_id)\n",
    "# We left-join one row per cart_id from original modeling dataset\n",
    "# (cart_id is ORDER_ID of that row)\n",
    "ctx_cols = [c for c in (ID_COLS + CUST_CTX_COLS + STORE_ONEHOTS + [\"total_order_price\"]) if c in modeling_header.columns]\n",
    "ctx = []\n",
    "\n",
    "for chunk in pd.read_csv(FILE_MODELING, usecols=ctx_cols, chunksize=100_000):\n",
    "    ctx.append(chunk.drop_duplicates(subset=[\"ORDER_ID\"]))\n",
    "ctx = pd.concat(ctx, ignore_index=True).drop_duplicates(subset=[\"ORDER_ID\"])\n",
    "ctx[\"ORDER_CREATED_DATE\"] = pd.to_datetime(ctx[\"ORDER_CREATED_DATE\"])\n",
    "\n",
    "train_df = train_df.merge(ctx.rename(columns={\"ORDER_ID\":\"cart_id\"}), on=\"cart_id\", how=\"left\")\n",
    "\n",
    "# Binary feature: candidate equals favorite item\n",
    "if \"favorite_item\" in train_df.columns:\n",
    "    train_df[\"cand_is_favorite_item\"] = (train_df[COL_CAND] == train_df[\"favorite_item\"].astype(str)).astype(int)\n",
    "else:\n",
    "    train_df[\"cand_is_favorite_item\"] = 0\n",
    "\n",
    "# ----- Consistent category vocab (train) -----\n",
    "CAT_COLS = [c for c in [\"ORDER_CHANNEL_NAME\",\"ORDER_SUBCHANNEL_NAME\",\"ORDER_OCCASION_NAME\"] if c in train_df.columns]\n",
    "cat_vocab = {}\n",
    "for c in CAT_COLS:\n",
    "    train_df[c] = train_df[c].astype(str)\n",
    "    cats = sorted(train_df[c].unique())\n",
    "    cat_vocab[c] = cats\n",
    "    map_c = {s:i for i,s in enumerate(cats)}\n",
    "    train_df[c] = train_df[c].map(map_c).fillna(-1).astype(int)\n",
    "\n",
    "with open(\"stage2_cat_vocab.json\",\"w\") as f:\n",
    "    json.dump(cat_vocab, f)\n",
    "\n",
    "# ----- Downcast numerics to float32 (saves RAM) -----\n",
    "for col in train_df.columns:\n",
    "    if col not in ('cart_id','label','ORDER_CREATED_DATE', COL_CAND):\n",
    "        if pd.api.types.is_numeric_dtype(train_df[col]):\n",
    "            train_df[col] = pd.to_numeric(train_df[col], downcast='float')\n",
    "\n",
    "# Final numeric cleanups\n",
    "for col in [\"cand_popularity\",\"jaccard\",\"lift\",COL_PROB,\"stage1_score\",\"stage1_rank\",\n",
    "            \"orders_count\",\"items_count\",\"repeat_purchase_rate\",\"avg_order_value\",\n",
    "            \"weekend_order_ratio\",\"store_diversity_count\",\"cart_size\",\"anchors_voted\"]:\n",
    "    if col in train_df.columns:\n",
    "        train_df[col] = train_df[col].fillna(0.0).astype(float)\n",
    "\n",
    "# Drop heavy strings not needed after feature derivation\n",
    "drop_cols = [\"candidate_bucket\",\"favorite_item\",\"most_common_store\"]\n",
    "for c in drop_cols:\n",
    "    if c in train_df.columns: train_df.drop(columns=[c], inplace=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b5d6a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['candidate_item', 'P_j_given_i', 'lift', 'jaccard', 'cooc_count',\n",
       "       'votes', 'popularity', 'P_j_given_i_norm', 'lift_norm', 'jaccard_norm',\n",
       "       'popularity_norm', 'stage1_score', 'label', 'cart_id',\n",
       "       'ORDER_CREATED_DATE_x', 'cart_size', 'anchors_voted', 'cart_has_combo',\n",
       "       'cart_has_wings', 'cart_has_fries', 'cart_has_dip', 'cand_popularity',\n",
       "       'stage1_rank', 'cand_is_combo', 'cand_is_fries', 'cand_is_dip',\n",
       "       'cand_is_wings', 'cand_is_strips', 'CUSTOMER_ID', 'STORE_NUMBER',\n",
       "       'ORDER_CREATED_DATE_y', 'ORDER_CHANNEL_NAME', 'ORDER_SUBCHANNEL_NAME',\n",
       "       'ORDER_OCCASION_NAME', 'total_order_price', 'cust_registered',\n",
       "       'cust_guest', 'cust_special_membership', 'store_STATE',\n",
       "       'store_city_Apple Valley', 'store_city_Ardmore', 'store_city_Arlington',\n",
       "       'store_city_Atwater', 'store_city_Aurora', 'store_city_Austin',\n",
       "       'store_city_Brandon', 'store_city_Charlotte', 'store_city_Cicero',\n",
       "       'store_city_Dallas', 'store_city_El Paso', 'store_city_Elsa',\n",
       "       'store_city_Grapevine', 'store_city_Henderson', 'store_city_Houston',\n",
       "       'store_city_Huntersville', 'store_city_Irving', 'store_city_Las Vegas',\n",
       "       'store_city_Laveen', 'store_city_Linden', 'store_city_Littleton',\n",
       "       'store_city_Macomb', 'store_city_Miami', 'store_city_Mililani',\n",
       "       'store_city_Nashville', 'store_city_Omaha', 'store_city_Oxnard',\n",
       "       'store_city_Rutherford', 'store_city_San Antonio',\n",
       "       'store_city_Scottsdale', 'store_city_Unknown', 'store_city_Walled Lake',\n",
       "       'store_city_Winter Park', 'orders_count', 'items_count',\n",
       "       'repeat_purchase_rate', 'avg_order_value', 'weekend_order_ratio',\n",
       "       'most_common_order_hour', 'most_common_order_dow',\n",
       "       'store_diversity_count', 'cand_is_favorite_item'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "359400f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-off coalesce for existing train_df\n",
    "if \"ORDER_CREATED_DATE\" not in train_df.columns:\n",
    "    if \"ORDER_CREATED_DATE_x\" in train_df.columns or \"ORDER_CREATED_DATE_y\" in train_df.columns:\n",
    "        left = train_df.get(\"ORDER_CREATED_DATE_x\")\n",
    "        right = train_df.get(\"ORDER_CREATED_DATE_y\")\n",
    "        train_df[\"ORDER_CREATED_DATE\"] = (left if left is not None else pd.Series(index=train_df.index)).fillna(right)\n",
    "for c in [\"ORDER_CREATED_DATE_ctx\", \"ORDER_CREATED_DATE_x\", \"ORDER_CREATED_DATE_y\"]:\n",
    "    if c in train_df.columns:\n",
    "        train_df.drop(columns=[c], inplace=True)\n",
    "train_df[\"ORDER_CREATED_DATE\"] = pd.to_datetime(train_df[\"ORDER_CREATED_DATE\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e32d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) groups sum must equal dataset length\n",
    "assert sum(train_group) == len(train_sorted), \"Train groups don't sum to num rows.\"\n",
    "assert sum(val_group) == len(val_sorted),     \"Val groups don't sum to num rows.\"\n",
    "\n",
    "# 2) features should be numeric (convert any stray objects just in case)\n",
    "bad = [c for c in FEATURES if not pd.api.types.is_numeric_dtype(train_sorted[c])]\n",
    "if bad:\n",
    "    for c in bad:\n",
    "        train_sorted[c] = pd.to_numeric(train_sorted[c], errors=\"coerce\").fillna(0)\n",
    "        val_sorted[c]   = pd.to_numeric(val_sorted[c], errors=\"coerce\").fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4635df",
   "metadata": {},
   "source": [
    "### Step 8 — Train/val split and LightGBM LambdaRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcccc7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttrain's ndcg@3: 0.671783\tval's ndcg@3: 0.68207\n",
      "[100]\ttrain's ndcg@3: 0.686826\tval's ndcg@3: 0.690709\n",
      "[150]\ttrain's ndcg@3: 0.696571\tval's ndcg@3: 0.696733\n",
      "[200]\ttrain's ndcg@3: 0.70662\tval's ndcg@3: 0.700724\n",
      "[250]\ttrain's ndcg@3: 0.712235\tval's ndcg@3: 0.703222\n",
      "[300]\ttrain's ndcg@3: 0.716669\tval's ndcg@3: 0.704836\n",
      "[350]\ttrain's ndcg@3: 0.721202\tval's ndcg@3: 0.704892\n",
      "[400]\ttrain's ndcg@3: 0.726643\tval's ndcg@3: 0.706856\n",
      "[450]\ttrain's ndcg@3: 0.731256\tval's ndcg@3: 0.707527\n",
      "[500]\ttrain's ndcg@3: 0.736165\tval's ndcg@3: 0.708223\n",
      "[550]\ttrain's ndcg@3: 0.739555\tval's ndcg@3: 0.70796\n",
      "[600]\ttrain's ndcg@3: 0.745449\tval's ndcg@3: 0.708546\n",
      "[650]\ttrain's ndcg@3: 0.751271\tval's ndcg@3: 0.710248\n",
      "[700]\ttrain's ndcg@3: 0.756842\tval's ndcg@3: 0.712066\n",
      "[750]\ttrain's ndcg@3: 0.761714\tval's ndcg@3: 0.713437\n",
      "[800]\ttrain's ndcg@3: 0.765087\tval's ndcg@3: 0.713459\n",
      "[850]\ttrain's ndcg@3: 0.76923\tval's ndcg@3: 0.714611\n",
      "[900]\ttrain's ndcg@3: 0.774652\tval's ndcg@3: 0.714981\n",
      "[950]\ttrain's ndcg@3: 0.779312\tval's ndcg@3: 0.715231\n",
      "[1000]\ttrain's ndcg@3: 0.784673\tval's ndcg@3: 0.715498\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[991]\ttrain's ndcg@3: 0.783991\tval's ndcg@3: 0.716129\n",
      "Saved model to stage2_lgbm_model.txt\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 8 — Train/val split and LightGBM LambdaRank =====\n",
    "\n",
    "import os, json\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Time-based split\n",
    "train_df[\"ORDER_CREATED_DATE\"] = pd.to_datetime(train_df[\"ORDER_CREATED_DATE\"])\n",
    "cutoff = train_df[\"ORDER_CREATED_DATE\"].quantile(1 - VAL_FRACTION_TIME)\n",
    "train_part = train_df[train_df[\"ORDER_CREATED_DATE\"] < cutoff].copy()\n",
    "val_part   = train_df[train_df[\"ORDER_CREATED_DATE\"] >= cutoff].copy()\n",
    "\n",
    "# 2) Feature list (exclude identifiers/labels)\n",
    "NON_FEATURES = {\n",
    "    \"cart_id\",\"label\",\"ORDER_CREATED_DATE\",\"candidate_item\",\n",
    "    \"CUSTOMER_ID\",\"STORE_NUMBER\"   # <— add these two\n",
    "}\n",
    "FEATURES = [c for c in train_df.columns if c not in NON_FEATURES]\n",
    "\n",
    "# Persist feature list for inference\n",
    "with open(FILE_STAGE2_FEATS, \"w\") as f:\n",
    "    json.dump(FEATURES, f)\n",
    "\n",
    "# 3) Ensure all FEATURES are numeric (coerce objects safely) and downcast to float32 to save RAM\n",
    "def ensure_numeric(df, cols):\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0.0\n",
    "        if not pd.api.types.is_numeric_dtype(df[c]):\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "    return df\n",
    "\n",
    "train_part_feat = ensure_numeric(train_part.copy(), FEATURES)\n",
    "val_part_feat   = ensure_numeric(val_part.copy(), FEATURES)\n",
    "\n",
    "# 4) Build LightGBM Datasets with group info (one group per cart)\n",
    "def to_lgb_dataset(df):\n",
    "    df = df.sort_values([\"cart_id\"]).copy()\n",
    "    X = df[FEATURES]\n",
    "    y = df[\"label\"].astype(int)\n",
    "    group_sizes = df.groupby(\"cart_id\").size().tolist()\n",
    "    dset = lgb.Dataset(X, label=y, free_raw_data=False)\n",
    "    return dset, group_sizes, df\n",
    "\n",
    "lgb_train, train_group, train_sorted = to_lgb_dataset(train_part_feat)\n",
    "lgb_val,   val_group,   val_sorted   = to_lgb_dataset(val_part_feat)\n",
    "\n",
    "# Set groups on datasets (required for ranking)\n",
    "lgb_train.set_group(train_group)\n",
    "lgb_val.set_group(val_group)\n",
    "\n",
    "# Sanity checks\n",
    "assert sum(train_group) == len(train_sorted), \"Train groups don't sum to num rows.\"\n",
    "assert sum(val_group)   == len(val_sorted),   \"Val groups don't sum to num rows.\"\n",
    "\n",
    "# 5) LightGBM params (LambdaRank) + speed/memory tweaks\n",
    "params = dict(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    ndcg_eval_at=[3],\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    min_data_in_leaf=50,\n",
    "    feature_fraction=0.9,\n",
    "    bagging_fraction=0.9,\n",
    "    bagging_freq=1,\n",
    "    max_depth=-1,\n",
    "    random_state=RANDOM_SEED,\n",
    "    verbose=-1,\n",
    "    # speed/memory\n",
    "    num_threads=os.cpu_count(),\n",
    "    max_bin=255,\n",
    "    min_data_in_bin=3,\n",
    ")\n",
    "\n",
    "# 6) Callbacks for early stopping + logging (works across LightGBM versions)\n",
    "callbacks = [\n",
    "    lgb.early_stopping(stopping_rounds=100, first_metric_only=False),\n",
    "    lgb.log_evaluation(period=50),\n",
    "]\n",
    "\n",
    "# 7) Train\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    valid_names=[\"train\", \"val\"],  # if your version errors on this, remove this line\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "# 8) Save model to disk\n",
    "model.save_model(FILE_STAGE2_MODEL)\n",
    "print(\"Saved model to\", FILE_STAGE2_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41005d4",
   "metadata": {},
   "source": [
    "### Step 9 — Validation Recall@K (1 & 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d233325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Recall@1=0.6397, Recall@3=0.7695\n"
     ]
    }
   ],
   "source": [
    "def recall_at_k(df_scored, k=3):\n",
    "    hits = 0\n",
    "    for cart_id, g in df_scored.groupby(\"cart_id\"):\n",
    "        topk = g.nlargest(k, \"score\")\n",
    "        if (topk[\"label\"] > 0).any():\n",
    "            hits += 1\n",
    "    return hits / df_scored[\"cart_id\"].nunique()\n",
    "\n",
    "# Score validation\n",
    "val_scores = model.predict(val_sorted[FEATURES], num_iteration=model.best_iteration)\n",
    "val_scored = val_sorted.copy()\n",
    "val_scored[\"score\"] = val_scores\n",
    "\n",
    "r1 = recall_at_k(val_scored, k=1)\n",
    "r3 = recall_at_k(val_scored, k=3)\n",
    "print(f\"Validation Recall@1={r1:.4f}, Recall@3={r3:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89e6c1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cand_is_favorite_item    2.114380e+06\n",
      "P_j_given_i_norm         8.821843e+05\n",
      "jaccard_norm             3.216634e+05\n",
      "stage1_rank              2.118754e+05\n",
      "total_order_price        2.095782e+05\n",
      "popularity_norm          1.218129e+05\n",
      "lift                     1.098521e+05\n",
      "P_j_given_i              8.897433e+04\n",
      "lift_norm                7.949733e+04\n",
      "popularity               7.548164e+04\n",
      "stage1_score             7.209091e+04\n",
      "jaccard                  7.205901e+04\n",
      "cooc_count               6.035629e+04\n",
      "items_count              5.636283e+04\n",
      "cart_has_wings           5.355803e+04\n",
      "repeat_purchase_rate     4.055914e+04\n",
      "cand_popularity          3.414247e+04\n",
      "avg_order_value          2.552053e+04\n",
      "cart_has_combo           2.515591e+04\n",
      "cand_is_combo            2.448984e+04\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "imp = pd.Series(model.feature_importance(importance_type=\"gain\"), index=FEATURES).sort_values(ascending=False).head(20)\n",
    "print(imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6edd76f",
   "metadata": {},
   "source": [
    "### Step 10 — Inference on test_data_question.csv → final Top-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be703bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(FILE_TEST)\n",
    "\n",
    "# find item columns (item1, item2, ...)\n",
    "ITEMCOLS_TEST = [c for c in test_df.columns if c.lower().startswith(\"item\")]\n",
    "ITEMCOLS_TEST = sorted(ITEMCOLS_TEST, key=lambda x: int(''.join(ch for ch in x if ch.isdigit()) or 0))\n",
    "\n",
    "# optional: join a compact customer/context lookup if you want personalization at inference\n",
    "# (build from modeling to avoid reloading the huge file)\n",
    "ctx_small = ctx.copy()  # from Step 7\n",
    "if \"CUSTOMER_ID\" in ctx_small.columns and \"CUSTOMER_ID\" in test_df.columns:\n",
    "    # best-effort de-dup\n",
    "    ctx_small = ctx_small.drop_duplicates(subset=[\"CUSTOMER_ID\"]).copy()\n",
    "\n",
    "def extract_test_cart(row):\n",
    "    items = []\n",
    "    for c in ITEMCOLS_TEST:\n",
    "        val = row.get(c)\n",
    "        if pd.notna(val) and str(val).strip():\n",
    "            items.append(canonicalize_item(val))\n",
    "    return items\n",
    "\n",
    "# Prepare feature list\n",
    "with open(FILE_STAGE2_FEATS, \"r\") as f:\n",
    "    FEATURES = json.load(f)\n",
    "\n",
    "rows_out = []\n",
    "for idx, row in test_df.iterrows():\n",
    "    cart_items = extract_test_cart(row)\n",
    "    cand = get_candidates_for_cart(cart_items).copy()\n",
    "    # basic features mirroring training\n",
    "    cand = cand.merge(pop_df.rename(columns={POPULAR_COL_ITEM: COL_CAND,\n",
    "                                             POPULAR_COL_FREQ: \"cand_popularity\"}),\n",
    "                      on=COL_CAND, how=\"left\")\n",
    "    cand[\"stage1_rank\"] = np.arange(1, len(cand)+1, dtype=int)\n",
    "    cand[\"candidate_bucket\"] = cand[COL_CAND].map(item_bucket)\n",
    "    cand[\"cand_is_combo\"]   = (cand[\"candidate_bucket\"] == \"combo\").astype(int)\n",
    "    cand[\"cand_is_fries\"]   = (cand[\"candidate_bucket\"] == \"fries\").astype(int)\n",
    "    cand[\"cand_is_dip\"]     = (cand[\"candidate_bucket\"] == \"dip\").astype(int)\n",
    "    cand[\"cand_is_wings\"]   = cand[\"candidate_bucket\"].isin([\"wings\",\"wings_spicy\",\"wings_grilled\"]).astype(int)\n",
    "    cand[\"cand_is_strips\"]  = (cand[\"candidate_bucket\"] == \"strips\").astype(int)\n",
    "    cand[\"cart_size\"] = len(cart_items)\n",
    "    cand[\"anchors_voted\"] = cand[COL_CAND].map(lambda c: sum([c in set(P_by_anchor.get(a, pd.DataFrame())[COL_CAND]) for a in cart_items]))\n",
    "    cand[\"cart_has_combo\"] = int(any(\"combo\" in it.lower() for it in cart_items))\n",
    "    cand[\"cart_has_wings\"] = int(any(\"wings\" in it.lower() for it in cart_items))\n",
    "    cand[\"cart_has_fries\"] = int(any(\"fries\" in it.lower() for it in cart_items))\n",
    "    cand[\"cart_has_dip\"]   = int(any(\"dip\"   in it.lower() for it in cart_items))\n",
    "\n",
    "    # attach context/personalization if available\n",
    "    if \"CUSTOMER_ID\" in test_df.columns and \"CUSTOMER_ID\" in ctx_small.columns:\n",
    "        row_ctx = ctx_small[ctx_small[\"CUSTOMER_ID\"] == row[\"CUSTOMER_ID\"]].head(1)\n",
    "        if not row_ctx.empty:\n",
    "            for c in [\"orders_count\",\"items_count\",\"repeat_purchase_rate\",\"avg_order_value\",\n",
    "                      \"weekend_order_ratio\",\"store_diversity_count\",\n",
    "                      \"cust_registered\",\"cust_guest\",\"cust_special_membership\"]:\n",
    "                if c in row_ctx.columns:\n",
    "                    cand[c] = float(row_ctx.iloc[0].get(c, 0.0))\n",
    "            # favorite item match\n",
    "            fav = str(row_ctx.iloc[0].get(\"favorite_item\", \"\")).strip()\n",
    "            cand[\"cand_is_favorite_item\"] = (cand[COL_CAND] == fav).astype(int)\n",
    "        else:\n",
    "            cand[\"cand_is_favorite_item\"] = 0\n",
    "    else:\n",
    "        cand[\"cand_is_favorite_item\"] = 0\n",
    "\n",
    "    # encode channel/occasion inputs\n",
    "    for c in [\"ORDER_CHANNEL_NAME\",\"ORDER_SUBCHANNEL_NAME\",\"ORDER_OCCASION_NAME\"]:\n",
    "        if c in test_df.columns:\n",
    "            # create deterministic mapping based on training categories (if known)\n",
    "            cand[c] = hash(str(row.get(c))) % 100  # simple numeric proxy\n",
    "\n",
    "    # numeric cleanups\n",
    "    for col in [\"cand_popularity\",\"jaccard\",\"lift\",COL_PROB,\"stage1_score\",\"stage1_rank\",\n",
    "                \"orders_count\",\"items_count\",\"repeat_purchase_rate\",\"avg_order_value\",\n",
    "                \"weekend_order_ratio\",\"store_diversity_count\",\"cart_size\",\"anchors_voted\"]:\n",
    "        if col in cand.columns:\n",
    "            cand[col] = cand[col].fillna(0.0).astype(float)\n",
    "\n",
    "    # Keep feature subset in correct order (missing columns auto-filled with 0)\n",
    "    for fcol in FEATURES:\n",
    "        if fcol not in cand.columns:\n",
    "            cand[fcol] = 0.0\n",
    "\n",
    "    import lightgbm as lgb\n",
    "    booster = lgb.Booster(model_file=FILE_STAGE2_MODEL)\n",
    "    scores = booster.predict(cand[FEATURES], num_iteration=booster.best_iteration if hasattr(booster,\"best_iteration\") else None)\n",
    "    cand[\"score\"] = scores\n",
    "\n",
    "    # Pick top-3 (you can add your soft-diversity selector here if you want)\n",
    "    top3 = cand.nlargest(3, \"score\")[COL_CAND].tolist()\n",
    "    while len(top3) < 3: top3.append(\"\")\n",
    "\n",
    "    rows_out.append({\n",
    "        \"ORDER_ID\": row[\"ORDER_ID\"],\n",
    "        \"RECOMMENDATION_1\": top3[0],\n",
    "        \"RECOMMENDATION_2\": top3[1],\n",
    "        \"RECOMMENDATION_3\": top3[2],\n",
    "    })\n",
    "\n",
    "stage2_out = pd.DataFrame(rows_out)\n",
    "stage2_out.to_csv(FILE_STAGE2_SUB, index=False)\n",
    "print(\"Saved:\", FILE_STAGE2_SUB)\n",
    "stage2_out.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
