{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd42d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# store_clean_encode.py\n",
    "# Run from terminal: python store_clean_encode.py --csv store_data.csv\n",
    "# Run in VS Code/Jupyter: just run the cell (it will ignore injected args)\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Robust imports (auto-install pgeocode if missing; sklearn is optional) ---\n",
    "try:\n",
    "    import pgeocode\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pgeocode\", \"-q\"], check=False)\n",
    "    import pgeocode\n",
    "\n",
    "try:\n",
    "    from sklearn.preprocessing import LabelEncoder  # optional\n",
    "except Exception:\n",
    "    LabelEncoder = None\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Config ----------\n",
    "COUNTRY_CODE = \"US\"\n",
    "nomi = pgeocode.Nominatim(COUNTRY_CODE)\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def safe_read_csv_local(path: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding=\"latin1\")\n",
    "\n",
    "def clean_postal(pc):\n",
    "    if pd.isna(pc):\n",
    "        return None\n",
    "    s = str(pc).strip()\n",
    "    m = re.search(r\"(\\d{5})\", s)\n",
    "    if m:\n",
    "        return m.group(1).zfill(5)\n",
    "    m2 = re.search(r\"(\\d{1,4})$\", s)\n",
    "    if m2:\n",
    "        return m2.group(1).zfill(5)\n",
    "    return None\n",
    "\n",
    "def norm_state(s):\n",
    "    if pd.isna(s): return None\n",
    "    s = str(s).strip().upper()\n",
    "    if s in (\"\", \"NAN\"): return None\n",
    "    m = re.match(r\"([A-Z]{2})\", s)  # keep first 2 letters if longer\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def norm_city(c):\n",
    "    if pd.isna(c): return None\n",
    "    c = str(c).strip()\n",
    "    if c == \"\" or c.lower() == \"nan\": return None\n",
    "    return c\n",
    "\n",
    "def safe_attr(obj, key):\n",
    "    try:\n",
    "        if obj is None: return None\n",
    "        if isinstance(obj, (pd.Series, dict)): return obj.get(key, None)\n",
    "        return getattr(obj, key, None)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# pgeocode caches (avoid repeated lookups)\n",
    "ZIP_CACHE = {}\n",
    "CITY_CACHE = {}\n",
    "\n",
    "def query_zip(zip5):\n",
    "    if not zip5: return None\n",
    "    if zip5 in ZIP_CACHE:\n",
    "        return ZIP_CACHE[zip5]\n",
    "    try:\n",
    "        rec = nomi.query_postal_code(zip5)\n",
    "    except Exception:\n",
    "        rec = None\n",
    "    ZIP_CACHE[zip5] = rec\n",
    "    return rec\n",
    "\n",
    "def query_city(city):\n",
    "    if not city: return None\n",
    "    if city in CITY_CACHE:\n",
    "        return CITY_CACHE[city]\n",
    "    try:\n",
    "        df = nomi.query_location(city)\n",
    "    except Exception:\n",
    "        df = None\n",
    "    CITY_CACHE[city] = df\n",
    "    return df\n",
    "\n",
    "def dominant_map(frame, key, val, min_share=0.90):\n",
    "    tmp = (frame.dropna(subset=[key, val])\n",
    "                 .groupby(key)[val]\n",
    "                 .value_counts(normalize=True)\n",
    "                 .rename(\"share\")\n",
    "                 .reset_index())\n",
    "    top = (tmp.sort_values([\"share\"], ascending=False)\n",
    "              .groupby(key)\n",
    "              .head(1))\n",
    "    top = top[top[\"share\"] >= min_share]\n",
    "    return top.set_index(key)[val].to_dict()\n",
    "\n",
    "def build_final(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Normalize inputs\n",
    "    na_values = [\"NA\", \"N/A\", \"nan\", \"NaN\", \"\"]\n",
    "    df = df.replace(na_values, np.nan).copy()\n",
    "\n",
    "    for col in [\"STORE_NUMBER\", \"CITY\", \"STATE\", \"POSTAL_CODE\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    df[\"STORE_NUMBER\"] = df[\"STORE_NUMBER\"].astype(str)\n",
    "    df[\"CITY\"]  = df[\"CITY\"].map(norm_city)\n",
    "    df[\"STATE\"] = df[\"STATE\"].map(norm_state)\n",
    "    df[\"POSTAL_CODE\"] = df[\"POSTAL_CODE\"].map(clean_postal)\n",
    "\n",
    "    # Dataset-driven fallbacks (only used if pgeocode fails)\n",
    "    zip2state_ds = dominant_map(df, \"POSTAL_CODE\", \"STATE\", min_share=0.90)\n",
    "    zip2city_ds  = dominant_map(df, \"POSTAL_CODE\", \"CITY\",  min_share=0.90)\n",
    "    city2state_ds= dominant_map(df, \"CITY\",        \"STATE\", min_share=0.90)\n",
    "\n",
    "    def fill_row(row):\n",
    "        city = norm_city(row.get(\"CITY\"))\n",
    "        state = norm_state(row.get(\"STATE\"))\n",
    "        zip5 = clean_postal(row.get(\"POSTAL_CODE\"))\n",
    "\n",
    "        # ZIP -> CITY/STATE via pgeocode\n",
    "        if zip5:\n",
    "            z = query_zip(zip5)\n",
    "            place  = norm_city(safe_attr(z, \"place_name\"))\n",
    "            stcode = norm_state(safe_attr(z, \"state_code\"))\n",
    "            if city is None and place:   city = place\n",
    "            if state is None and stcode: state = stcode\n",
    "            row[\"POSTAL_CODE\"] = zip5  # keep normalized\n",
    "\n",
    "        # CITY -> ZIP/STATE via pgeocode\n",
    "        if city:\n",
    "            cand = query_city(city)\n",
    "            if isinstance(cand, pd.DataFrame) and not cand.empty:\n",
    "                if \"state_code\" in cand.columns and state:\n",
    "                    st_series = cand[\"state_code\"].astype(str).str.upper().fillna(\"\")\n",
    "                    cand = cand[st_series == state]\n",
    "                if \"population\" in cand.columns:\n",
    "                    cand = cand.sort_values(\"population\", ascending=False)\n",
    "                chosen = None\n",
    "                for _, r in cand.iterrows():\n",
    "                    pc = r.get(\"postal_code\", None)\n",
    "                    st = r.get(\"state_code\", None)\n",
    "                    if pd.notnull(pc) or pd.notnull(st):\n",
    "                        chosen = r\n",
    "                        break\n",
    "                if chosen is not None:\n",
    "                    if pd.isna(row.get(\"POSTAL_CODE\")):\n",
    "                        pc = chosen.get(\"postal_code\", None)\n",
    "                        if pd.notnull(pc):\n",
    "                            row[\"POSTAL_CODE\"] = clean_postal(pc)\n",
    "                    if state is None and pd.notnull(chosen.get(\"state_code\", None)):\n",
    "                        state = norm_state(chosen.get(\"state_code\", None))\n",
    "\n",
    "        # Dataset fallbacks (only when still missing)\n",
    "        if state is None and zip5 and zip5 in zip2state_ds:\n",
    "            state = norm_state(zip2state_ds[zip5])\n",
    "        if city is None and zip5 and zip5 in zip2city_ds:\n",
    "            city = norm_city(zip2city_ds[zip5])\n",
    "        if state is None and city and city in city2state_ds:\n",
    "            state = norm_state(city2state_ds[city])\n",
    "\n",
    "        row[\"CITY\"] = city if city is not None else row.get(\"CITY\")\n",
    "        row[\"STATE\"] = state if state is not None else row.get(\"STATE\")\n",
    "        return row\n",
    "\n",
    "    df = df.apply(fill_row, axis=1)\n",
    "\n",
    "    # Mark completely empty location rows as Unknown/UNK\n",
    "    mask_all_null = df[[\"CITY\", \"STATE\", \"POSTAL_CODE\"]].isnull().all(axis=1)\n",
    "    df.loc[mask_all_null, \"STATE\"] = \"UNK\"\n",
    "    df.loc[mask_all_null, \"CITY\"]  = \"Unknown\"\n",
    "\n",
    "    # Final normalization\n",
    "    df[\"CITY\"]  = df[\"CITY\"].map(lambda x: str(x).upper() if pd.notnull(x) else x)\n",
    "    df[\"STATE\"] = df[\"STATE\"].map(lambda x: str(x).upper()[:2] if pd.notnull(x) else x)\n",
    "    df[\"POSTAL_CODE\"] = df[\"POSTAL_CODE\"].map(lambda x: clean_postal(x) if pd.notnull(x) else x)\n",
    "\n",
    "    # Encode + OHE (original output schema)\n",
    "    if LabelEncoder is not None:\n",
    "        try:\n",
    "            enc = LabelEncoder()\n",
    "            df[\"STORE_NUMBER_ENC\"] = enc.fit_transform(df[\"STORE_NUMBER\"].astype(str))\n",
    "        except Exception:\n",
    "            df[\"STORE_NUMBER_ENC\"] = pd.factorize(df[\"STORE_NUMBER\"].astype(str))[0]\n",
    "    else:\n",
    "        df[\"STORE_NUMBER_ENC\"] = pd.factorize(df[\"STORE_NUMBER\"].astype(str))[0]\n",
    "\n",
    "    state_ohe = pd.get_dummies(df[\"STATE\"].fillna(\"UNK\"), prefix=\"state\").astype(int)\n",
    "    city_ohe  = pd.get_dummies(df[\"CITY\"].fillna(\"Unknown\"), prefix=\"city\").astype(int)\n",
    "\n",
    "    final_df = pd.concat([\n",
    "        df[[\"STORE_NUMBER\", \"STORE_NUMBER_ENC\", \"STATE\"]],\n",
    "        state_ohe,\n",
    "        city_ohe\n",
    "    ], axis=1)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--csv\", default=\"store_data.csv\",\n",
    "                        help=\"Input CSV filename located in the same folder as this script\")\n",
    "\n",
    "    # Ignore VS Code/Jupyter's injected args like --f=...\n",
    "    ns, _ = parser.parse_known_args()\n",
    "\n",
    "    # Resolve paths relative to this script; fallback to CWD in notebooks\n",
    "    base_dir = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
    "\n",
    "    in_path = (base_dir / ns.csv).resolve()\n",
    "    if not in_path.exists():\n",
    "        raise FileNotFoundError(f\"Input file not found at: {in_path}\")\n",
    "\n",
    "    print(f\"Reading: {in_path}\")\n",
    "    df = safe_read_csv_local(in_path)\n",
    "    final_df = build_final(df)\n",
    "\n",
    "    out_path = base_dir / \"store_data_cleaned_and_encoded.csv\"\n",
    "    final_df.to_csv(out_path, index=False)\n",
    "    print(f\"Written {out_path} with shape {final_df.shape}\")\n",
    "    print(\"✅ store_data_cleaning: Completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
