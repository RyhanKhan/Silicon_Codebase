{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b01a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, gc, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === Files (same folder) ===\n",
    "FILE_TEST     = \"test_data_question.csv\"\n",
    "FILE_P        = \"P_j_given_i.csv\"\n",
    "FILE_LIFT     = \"item_lift_matrix.csv\"\n",
    "FILE_JACC     = \"item_jaccard_matrix.csv\"\n",
    "FILE_COOC     = \"item_cooccurrence_counts.csv\"\n",
    "FILE_POP      = \"item_stats_counts_and_freq.csv\"\n",
    "\n",
    "FILE_STAGE2_MODEL = \"stage2_lgbm_model.txt\"\n",
    "FILE_STAGE2_FEATS = \"stage2_feature_columns.json\"   # try to load; fall back to model feature names\n",
    "FILE_CAT_VOCAB    = \"stage2_cat_vocab.json\"         # try to load; fall back to test-driven codes\n",
    "\n",
    "FILE_MODELING     = \"modeling_dataset.csv\"          # optional: only to build a tiny ctx lookup\n",
    "\n",
    "# === Stage-1 knobs (safe defaults) ===\n",
    "TOP_N_PER_ANCHOR   = 60\n",
    "FALLBACK_MIN_CANDS = 40\n",
    "AGG_PROB_METHOD    = \"one_minus_prod\"   # {\"one_minus_prod\",\"sum\",\"max\"}\n",
    "AGG_OTHER_METHOD   = \"mean\"             # {\"mean\",\"sum\",\"max\"}\n",
    "W_PROB, W_LIFT, W_JACC, W_POPU = 0.60, 0.25, 0.10, 0.05\n",
    "\n",
    "# === Popularity columns (from your file) ===\n",
    "POPULAR_COL_ITEM = \"Unnamed: 0\"\n",
    "POPULAR_COL_FREQ = \"freq\"\n",
    "\n",
    "# === Column aliases ===\n",
    "COL_ANCHOR = \"anchor_item\"\n",
    "COL_CAND   = \"candidate_item\"\n",
    "COL_PROB   = \"P_j_given_i\"\n",
    "\n",
    "def canonicalize_item(x):\n",
    "    if pd.isna(x): return x\n",
    "    return str(x).strip()\n",
    "\n",
    "def safe_minmax(s: pd.Series):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\").fillna(0.0).astype(float)\n",
    "    lo, hi = s.min(), s.max()\n",
    "    if hi - lo < 1e-12:\n",
    "        return pd.Series(np.zeros(len(s), dtype=float), index=s.index)\n",
    "    return (s - lo) / (hi - lo)\n",
    "\n",
    "def one_minus_product_of_complements(values):\n",
    "    v = np.clip(np.array(values, dtype=float), 0.0, 1.0)\n",
    "    return float(1.0 - np.prod(1.0 - v))\n",
    "\n",
    "def item_bucket(name: str) -> str:\n",
    "    n = str(name).lower()\n",
    "    if \"fries\" in n: return \"fries\"\n",
    "    if \"combo\" in n: return \"combo\"\n",
    "    if \"dip\" in n: return \"dip\"\n",
    "    if \"corn\" in n: return \"sides\"\n",
    "    if any(w in n for w in [\"cake\",\"brownie\",\"cookie\"]): return \"dessert\"\n",
    "    if \"drink\" in n or \"soda\" in n: return \"drink\"\n",
    "    if \"wings\" in n and \"spicy\" in n: return \"wings_spicy\"\n",
    "    if \"wings\" in n and \"grilled\" in n: return \"wings_grilled\"\n",
    "    if \"wings\" in n: return \"wings\"\n",
    "    if \"strips\" in n: return \"strips\"\n",
    "    return \"other\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "978ab567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage-1 assets ready. Anchors: 130\n"
     ]
    }
   ],
   "source": [
    "# 1) Popularity\n",
    "pop_df = pd.read_csv(FILE_POP)\n",
    "pop_df = pop_df[[POPULAR_COL_ITEM, POPULAR_COL_FREQ]].dropna()\n",
    "pop_df[POPULAR_COL_ITEM] = pop_df[POPULAR_COL_ITEM].map(canonicalize_item)\n",
    "pop_df = pop_df.sort_values(POPULAR_COL_FREQ, ascending=False).reset_index(drop=True)\n",
    "\n",
    "# 2) P(j|i) â€” wide or long\n",
    "P_raw = pd.read_csv(FILE_P)\n",
    "if COL_PROB not in P_raw.columns or 'item' in P_raw.columns or 'Item' in P_raw.columns:\n",
    "    row_item_col = 'item' if 'item' in P_raw.columns else ('Item' if 'Item' in P_raw.columns else P_raw.columns[0])\n",
    "    P = P_raw.melt(id_vars=[row_item_col], var_name=COL_CAND, value_name=COL_PROB)\\\n",
    "             .rename(columns={row_item_col: COL_ANCHOR})\n",
    "else:\n",
    "    P = P_raw.rename(columns={P_raw.columns[0]: COL_ANCHOR,\n",
    "                              P_raw.columns[1]: COL_CAND,\n",
    "                              P_raw.columns[2]: COL_PROB})\n",
    "P[COL_ANCHOR] = P[COL_ANCHOR].map(canonicalize_item)\n",
    "P[COL_CAND]   = P[COL_CAND].map(canonicalize_item)\n",
    "P = P.dropna(subset=[COL_CAND, COL_PROB])\n",
    "\n",
    "# 3) Melt lift/jaccard/cooc if needed\n",
    "def to_long_pairs(df, value_col):\n",
    "    cols = df.columns.tolist()\n",
    "    str_cols = [c for c in cols if df[c].dtype == \"object\"]\n",
    "    if value_col in cols and len(str_cols) >= 2:\n",
    "        return df.rename(columns={str_cols[0]: COL_ANCHOR, str_cols[1]: COL_CAND})\n",
    "    base = df.copy()\n",
    "    row_item_col = cols[0]\n",
    "    base[row_item_col] = base[row_item_col].map(canonicalize_item)\n",
    "    long_df = base.melt(id_vars=[row_item_col], var_name=COL_CAND, value_name=value_col)\\\n",
    "                  .rename(columns={row_item_col: COL_ANCHOR})\n",
    "    long_df[COL_CAND] = long_df[COL_CAND].map(canonicalize_item)\n",
    "    return long_df\n",
    "\n",
    "lift_pairs = to_long_pairs(pd.read_csv(FILE_LIFT),  \"lift\")\n",
    "jacc_pairs = to_long_pairs(pd.read_csv(FILE_JACC),  \"jaccard\")\n",
    "cooc_pairs = to_long_pairs(pd.read_csv(FILE_COOC),  \"cooc_count\")\n",
    "\n",
    "lift_pairs = lift_pairs[[COL_ANCHOR, COL_CAND, \"lift\"]].dropna()\n",
    "jacc_pairs = jacc_pairs[[COL_ANCHOR, COL_CAND, \"jaccard\"]].dropna()\n",
    "cooc_pairs = cooc_pairs[[COL_ANCHOR, COL_CAND, \"cooc_count\"]].dropna()\n",
    "\n",
    "# 4) Build per-anchor Top-N and prejoin signals ONCE\n",
    "P_sorted = P.sort_values([COL_ANCHOR, COL_PROB], ascending=[True, False]).copy()\n",
    "P_topN = P_sorted.groupby(COL_ANCHOR, as_index=False).head(TOP_N_PER_ANCHOR)\n",
    "\n",
    "pairs_all = P_topN.merge(lift_pairs, on=[COL_ANCHOR, COL_CAND], how=\"left\") \\\n",
    "                  .merge(jacc_pairs, on=[COL_ANCHOR, COL_CAND], how=\"left\") \\\n",
    "                  .merge(cooc_pairs, on=[COL_ANCHOR, COL_CAND], how=\"left\")\n",
    "\n",
    "CAND_BY_ANCHOR = {a: g[[COL_CAND, COL_PROB, \"lift\", \"jaccard\", \"cooc_count\"]].reset_index(drop=True)\n",
    "                  for a, g in pairs_all.groupby(COL_ANCHOR)}\n",
    "\n",
    "POPULAR_ITEMS = pop_df[POPULAR_COL_ITEM].tolist()\n",
    "print(\"Stage-1 assets ready. Anchors:\", len(CAND_BY_ANCHOR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fd00134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates_for_cart(cart_items):\n",
    "    frames = []\n",
    "    for anchor in cart_items:\n",
    "        df = CAND_BY_ANCHOR.get(anchor)\n",
    "        if df is not None and not df.empty:\n",
    "            tmp = df.copy()\n",
    "            tmp[\"_anchor\"] = anchor\n",
    "            frames.append(tmp)\n",
    "\n",
    "    if not frames:\n",
    "        fb = pop_df[~pop_df[POPULAR_COL_ITEM].isin(cart_items)].rename(columns={POPULAR_COL_ITEM: COL_CAND}).copy()\n",
    "        fb[\"stage1_score\"] = safe_minmax(fb[POPULAR_COL_FREQ])\n",
    "        fb[\"votes\"] = 0\n",
    "        return fb[[COL_CAND, \"stage1_score\", \"votes\"]].head(FALLBACK_MIN_CANDS)\n",
    "\n",
    "    cand_pairs = pd.concat(frames, ignore_index=True)\n",
    "    cand_pairs = cand_pairs[~cand_pairs[COL_CAND].isin(cart_items)].copy()\n",
    "\n",
    "    def agg_prob(s):\n",
    "        if AGG_PROB_METHOD == \"one_minus_prod\": return one_minus_product_of_complements(s)\n",
    "        if AGG_PROB_METHOD == \"sum\": return s.sum()\n",
    "        return s.max()\n",
    "    def agg_other(s):\n",
    "        if AGG_OTHER_METHOD == \"mean\": return s.mean(skipna=True)\n",
    "        if AGG_OTHER_METHOD == \"sum\": return s.sum(skipna=True)\n",
    "        return s.max(skipna=True)\n",
    "\n",
    "    grouped = cand_pairs.groupby(COL_CAND).agg({\n",
    "        COL_PROB: agg_prob,\n",
    "        \"lift\": agg_other,\n",
    "        \"jaccard\": agg_other,\n",
    "        \"cooc_count\": \"sum\",\n",
    "        \"_anchor\": \"count\"\n",
    "    }).rename(columns={\"_anchor\": \"votes\"}).reset_index()\n",
    "\n",
    "    grouped = grouped.merge(\n",
    "        pop_df.rename(columns={POPULAR_COL_ITEM: COL_CAND, POPULAR_COL_FREQ: \"popularity\"}),\n",
    "        on=COL_CAND, how=\"left\"\n",
    "    )\n",
    "\n",
    "    for col in [COL_PROB, \"lift\", \"jaccard\", \"popularity\"]:\n",
    "        if col not in grouped.columns: grouped[col] = 0.0\n",
    "        grouped[f\"{col}_norm\"] = safe_minmax(grouped[col])\n",
    "\n",
    "    grouped[\"stage1_score\"] = (\n",
    "        W_PROB*grouped[f\"{COL_PROB}_norm\"] +\n",
    "        W_LIFT*grouped[\"lift_norm\"] +\n",
    "        W_JACC*grouped[\"jaccard_norm\"] +\n",
    "        W_POPU*grouped[\"popularity_norm\"]\n",
    "    )\n",
    "\n",
    "    need = max(0, FALLBACK_MIN_CANDS - len(grouped))\n",
    "    if need > 0:\n",
    "        fb = pop_df[~pop_df[POPULAR_COL_ITEM].isin(set(grouped[COL_CAND]).union(cart_items))] \\\n",
    "                  .head(need).rename(columns={POPULAR_COL_ITEM: COL_CAND})\n",
    "        fb[\"stage1_score\"] = 0.0\n",
    "        fb[\"votes\"] = 0\n",
    "        grouped = pd.concat([grouped[[COL_CAND,\"stage1_score\",\"votes\",COL_PROB,\"lift\",\"jaccard\",\"popularity\"]],\n",
    "                             fb[[COL_CAND,\"stage1_score\",\"votes\"]]],\n",
    "                            ignore_index=True)\n",
    "\n",
    "    return grouped.sort_values(\"stage1_score\", ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a367f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features to use: 74\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Load booster\n",
    "booster = lgb.Booster(model_file=FILE_STAGE2_MODEL)\n",
    "\n",
    "# Features: try saved list, else from model\n",
    "if os.path.exists(FILE_STAGE2_FEATS):\n",
    "    with open(FILE_STAGE2_FEATS, \"r\") as f:\n",
    "        FEATURES = json.load(f)\n",
    "else:\n",
    "    # fallback to model feature names\n",
    "    FEATURES = list(booster.feature_name())\n",
    "print(\"Features to use:\", len(FEATURES))\n",
    "\n",
    "# Category vocab (train->test mapping); fall back to test-only codes\n",
    "cat2id = {}\n",
    "if os.path.exists(FILE_CAT_VOCAB):\n",
    "    with open(FILE_CAT_VOCAB, \"r\") as f:\n",
    "        cat_vocab = json.load(f)\n",
    "    cat2id = {c:{s:i for i,s in enumerate(v)} for c,v in cat_vocab.items()}\n",
    "else:\n",
    "    print(\"WARNING: category vocab not found. Will build codes from test values (may reduce accuracy).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb81d6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built ctx_small with rows: 563346\n"
     ]
    }
   ],
   "source": [
    "# Build a slim per-customer lookup to personalize a bit (safe subset of columns)\n",
    "ctx_small = None\n",
    "try:\n",
    "    base_cols = [\"CUSTOMER_ID\",\"orders_count\",\"items_count\",\"repeat_purchase_rate\",\n",
    "                 \"avg_order_value\",\"weekend_order_ratio\",\"store_diversity_count\",\n",
    "                 \"cust_registered\",\"cust_guest\",\"cust_special_membership\",\"store_STATE\"]\n",
    "    # keep only available\n",
    "    preview = pd.read_csv(FILE_MODELING, nrows=5)\n",
    "    usecols = [c for c in base_cols if c in preview.columns]\n",
    "    cs = []\n",
    "    for chunk in pd.read_csv(FILE_MODELING, usecols=usecols, chunksize=200_000):\n",
    "        cs.append(chunk.drop_duplicates(subset=[\"CUSTOMER_ID\"]))\n",
    "    ctx_small = pd.concat(cs, ignore_index=True).drop_duplicates(subset=[\"CUSTOMER_ID\"])\n",
    "    print(\"Built ctx_small with rows:\", len(ctx_small))\n",
    "except Exception as e:\n",
    "    print(\"Skipping ctx_small build:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ff23d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: stage2_recommendations_top3.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORDER_ID</th>\n",
       "      <th>RECOMMENDATION_1</th>\n",
       "      <th>RECOMMENDATION_2</th>\n",
       "      <th>RECOMMENDATION_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9351345556</td>\n",
       "      <td>Chicken Sub</td>\n",
       "      <td>Add 5 Spicy Wings</td>\n",
       "      <td>Regular Buffalo Fries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3595377080</td>\n",
       "      <td>Ranch Dip - Regular</td>\n",
       "      <td>Blue Cheese Dip - Regular</td>\n",
       "      <td>2 pc Crispy Strips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4071757785</td>\n",
       "      <td>Regular Buffalo Fries</td>\n",
       "      <td>Add 5 Spicy Wings</td>\n",
       "      <td>Add 5 Grilled Wings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3931766769</td>\n",
       "      <td>Ranch Dip - Regular</td>\n",
       "      <td>Regular Buffalo Fries</td>\n",
       "      <td>Ranch Dip - Large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3739700809</td>\n",
       "      <td>Ranch Dip - Regular</td>\n",
       "      <td>Large Buffalo Fries</td>\n",
       "      <td>Fried Corn - Regular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ORDER_ID       RECOMMENDATION_1           RECOMMENDATION_2  \\\n",
       "0  9351345556            Chicken Sub          Add 5 Spicy Wings   \n",
       "1  3595377080    Ranch Dip - Regular  Blue Cheese Dip - Regular   \n",
       "2  4071757785  Regular Buffalo Fries          Add 5 Spicy Wings   \n",
       "3  3931766769    Ranch Dip - Regular      Regular Buffalo Fries   \n",
       "4  3739700809    Ranch Dip - Regular        Large Buffalo Fries   \n",
       "\n",
       "        RECOMMENDATION_3  \n",
       "0  Regular Buffalo Fries  \n",
       "1     2 pc Crispy Strips  \n",
       "2    Add 5 Grilled Wings  \n",
       "3      Ranch Dip - Large  \n",
       "4   Fried Corn - Regular  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read test\n",
    "test_df = pd.read_csv(FILE_TEST)\n",
    "\n",
    "# item1, item2, ...\n",
    "ITEMCOLS_TEST = [c for c in test_df.columns if c.lower().startswith(\"item\")]\n",
    "ITEMCOLS_TEST = sorted(ITEMCOLS_TEST, key=lambda x: int(''.join(ch for ch in x if ch.isdigit()) or 0))\n",
    "\n",
    "def extract_test_cart(row):\n",
    "    items = []\n",
    "    for c in ITEMCOLS_TEST:\n",
    "        v = row.get(c)\n",
    "        if pd.notna(v) and str(v).strip():\n",
    "            items.append(canonicalize_item(v))\n",
    "    return items\n",
    "\n",
    "rows_out = []\n",
    "for _, row in test_df.iterrows():\n",
    "    cart_items = extract_test_cart(row)\n",
    "    cand = get_candidates_for_cart(cart_items).copy()\n",
    "\n",
    "    # Candidate attributes / basics\n",
    "    cand = cand.merge(pop_df.rename(columns={POPULAR_COL_ITEM: COL_CAND, POPULAR_COL_FREQ: \"cand_popularity\"}),\n",
    "                      on=COL_CAND, how=\"left\")\n",
    "    cand[\"stage1_rank\"] = np.arange(1, len(cand)+1, dtype=int)\n",
    "    cand[\"candidate_bucket\"] = cand[COL_CAND].map(item_bucket)\n",
    "    cand[\"cand_is_combo\"]   = (cand[\"candidate_bucket\"] == \"combo\").astype(int)\n",
    "    cand[\"cand_is_fries\"]   = (cand[\"candidate_bucket\"] == \"fries\").astype(int)\n",
    "    cand[\"cand_is_dip\"]     = (cand[\"candidate_bucket\"] == \"dip\").astype(int)\n",
    "    cand[\"cand_is_wings\"]   = cand[\"candidate_bucket\"].isin([\"wings\",\"wings_spicy\",\"wings_grilled\"]).astype(int)\n",
    "    cand[\"cand_is_strips\"]  = (cand[\"candidate_bucket\"] == \"strips\").astype(int)\n",
    "    cand[\"cart_size\"] = len(cart_items)\n",
    "    cand[\"anchors_voted\"] = cand.get(\"votes\", 0)\n",
    "\n",
    "    cand[\"cart_has_combo\"] = int(any(\"combo\" in it.lower() for it in cart_items))\n",
    "    cand[\"cart_has_wings\"] = int(any(\"wings\" in it.lower() for it in cart_items))\n",
    "    cand[\"cart_has_fries\"] = int(any(\"fries\" in it.lower() for it in cart_items))\n",
    "    cand[\"cart_has_dip\"]   = int(any(\"dip\"   in it.lower() for it in cart_items))\n",
    "\n",
    "    # Personalization (if lookup available)\n",
    "    if ctx_small is not None and \"CUSTOMER_ID\" in test_df.columns and \"CUSTOMER_ID\" in ctx_small.columns:\n",
    "        rc = ctx_small[ctx_small[\"CUSTOMER_ID\"] == row.get(\"CUSTOMER_ID\")].head(1)\n",
    "        if not rc.empty:\n",
    "            for c in [\"orders_count\",\"items_count\",\"repeat_purchase_rate\",\"avg_order_value\",\n",
    "                      \"weekend_order_ratio\",\"store_diversity_count\",\n",
    "                      \"cust_registered\",\"cust_guest\",\"cust_special_membership\"]:\n",
    "                if c in rc.columns:\n",
    "                    cand[c] = float(rc.iloc[0].get(c, 0.0))\n",
    "            if \"store_STATE\" in rc.columns and \"store_STATE\" in cat2id:\n",
    "                s = str(rc.iloc[0].get(\"store_STATE\",\"\"))\n",
    "                cand[\"store_STATE\"] = cat2id[\"store_STATE\"].get(s, -1)\n",
    "        else:\n",
    "            for c in [\"orders_count\",\"items_count\",\"repeat_purchase_rate\",\"avg_order_value\",\n",
    "                      \"weekend_order_ratio\",\"store_diversity_count\",\n",
    "                      \"cust_registered\",\"cust_guest\",\"cust_special_membership\",\"store_STATE\"]:\n",
    "                cand[c] = 0.0\n",
    "    else:\n",
    "        # No personalization\n",
    "        for c in [\"orders_count\",\"items_count\",\"repeat_purchase_rate\",\"avg_order_value\",\n",
    "                  \"weekend_order_ratio\",\"store_diversity_count\",\n",
    "                  \"cust_registered\",\"cust_guest\",\"cust_special_membership\",\"store_STATE\"]:\n",
    "            cand[c] = 0.0\n",
    "\n",
    "    # Encode channel/occasion with saved vocab (or fallback)\n",
    "    for c in [\"ORDER_CHANNEL_NAME\",\"ORDER_SUBCHANNEL_NAME\",\"ORDER_OCCASION_NAME\"]:\n",
    "        if c in test_df.columns:\n",
    "            if cat2id:\n",
    "                code = cat2id.get(c, {}).get(str(row.get(c, \"\")), -1)\n",
    "            else:\n",
    "                # fallback: build simple per-test mapping (consistent within this run)\n",
    "                # map unseen to -1\n",
    "                # (You can prebuild dicts outside loop for speed if needed.)\n",
    "                code = hash(str(row.get(c, \"\"))) % 100\n",
    "            cand[c] = int(code)\n",
    "\n",
    "    # Drop helper col and ensure FEATURES exist & numeric\n",
    "    if \"candidate_bucket\" in cand.columns:\n",
    "        cand.drop(columns=[\"candidate_bucket\"], inplace=True)\n",
    "\n",
    "    for fcol in FEATURES:\n",
    "        if fcol not in cand.columns:\n",
    "            cand[fcol] = 0.0\n",
    "        if not pd.api.types.is_numeric_dtype(cand[fcol]):\n",
    "            cand[fcol] = pd.to_numeric(cand[fcol], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # Predict with best_iteration\n",
    "    scores = booster.predict(cand[FEATURES], num_iteration=getattr(booster, \"best_iteration\", None))\n",
    "    cand[\"score\"] = scores\n",
    "\n",
    "    # Top-3\n",
    "    top3 = cand.nlargest(3, \"score\")[COL_CAND].tolist()\n",
    "    # ensure uniqueness and not already in cart (belt-and-suspenders)\n",
    "    seen = set()\n",
    "    out3 = []\n",
    "    for it in top3:\n",
    "        if it not in seen and it not in cart_items:\n",
    "            out3.append(it); seen.add(it)\n",
    "        if len(out3) == 3: break\n",
    "    while len(out3) < 3: out3.append(\"\")\n",
    "\n",
    "    rows_out.append({\n",
    "        \"ORDER_ID\": row[\"ORDER_ID\"],\n",
    "        \"RECOMMENDATION_1\": out3[0],\n",
    "        \"RECOMMENDATION_2\": out3[1],\n",
    "        \"RECOMMENDATION_3\": out3[2],\n",
    "    })\n",
    "\n",
    "stage2_out = pd.DataFrame(rows_out)\n",
    "OUT_FILE = \"stage2_recommendations_top3.csv\"\n",
    "stage2_out.to_csv(OUT_FILE, index=False)\n",
    "print(\"Saved:\", OUT_FILE)\n",
    "stage2_out.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40b46955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv(\"test_data_question.csv\")\n",
    "sub  = pd.read_csv(\"stage2_recommendations_top3.csv\")\n",
    "sub  = test[[\"ORDER_ID\"]].merge(sub, on=\"ORDER_ID\", how=\"left\")\n",
    "sub.to_excel(\"stage2_recommendations_top3.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85fc42c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in test: 1000\n",
      "Rows in merged: 1000\n",
      "Missing rec rows: 0\n",
      "Blank strings in recs: 0\n",
      "Saved: submission_test_data_question.csv\n",
      "Saved: submission_test_data_question.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === file names (same folder) ===\n",
    "TEST_PATH = \"test_data_question.csv\"\n",
    "RECS_PATH = \"stage2_recommendations_top3.csv\"\n",
    "OUT_CSV   = \"submission_test_data_question.csv\"\n",
    "OUT_XLSX  = \"submission_test_data_question.xlsx\"\n",
    "\n",
    "# 1) Load\n",
    "test = pd.read_csv(TEST_PATH, dtype={\"ORDER_ID\": str, \"CUSTOMER_ID\": str})\n",
    "recs = pd.read_csv(RECS_PATH, dtype={\"ORDER_ID\": str})\n",
    "\n",
    "# 2) Standardize recommendation column names (underscore -> space)\n",
    "rename_map = {\n",
    "    \"RECOMMENDATION_1\": \"RECOMMENDATION 1\",\n",
    "    \"RECOMMENDATION_2\": \"RECOMMENDATION 2\",\n",
    "    \"RECOMMENDATION_3\": \"RECOMMENDATION 3\",\n",
    "}\n",
    "# If they already have spaces, this is a no-op\n",
    "recs = recs.rename(columns=rename_map)\n",
    "\n",
    "# 3) Ensure we only keep the needed rec columns + ORDER_ID\n",
    "rec_cols = [\"RECOMMENDATION 1\", \"RECOMMENDATION 2\", \"RECOMMENDATION 3\"]\n",
    "keep_cols = [\"ORDER_ID\"] + [c for c in rec_cols if c in recs.columns]\n",
    "recs = recs[keep_cols].drop_duplicates(subset=[\"ORDER_ID\"], keep=\"first\")\n",
    "\n",
    "# 4) Left-join onto test (keeps test order)\n",
    "merged = test.merge(recs, on=\"ORDER_ID\", how=\"left\")\n",
    "\n",
    "# 5) Put recommendation columns at the end (after all original test columns)\n",
    "ordered_cols = [c for c in merged.columns if c not in rec_cols] + rec_cols\n",
    "merged = merged[ordered_cols]\n",
    "\n",
    "# 6) Fill any missing recs with blank strings (submission-safe)\n",
    "for c in rec_cols:\n",
    "    if c not in merged.columns:\n",
    "        merged[c] = \"\"\n",
    "    merged[c] = merged[c].fillna(\"\")\n",
    "\n",
    "# 7) Quick sanity checks (optional prints)\n",
    "print(\"Rows in test:\", len(test))\n",
    "print(\"Rows in merged:\", len(merged))\n",
    "print(\"Missing rec rows:\", int(merged[rec_cols].isna().any(axis=1).sum()))\n",
    "print(\"Blank strings in recs:\", int((merged[rec_cols] == \"\").sum().sum()))\n",
    "\n",
    "# 8) Save outputs\n",
    "merged.to_csv(OUT_CSV, index=False)\n",
    "merged.to_excel(OUT_XLSX, index=False)\n",
    "\n",
    "print(\"Saved:\", OUT_CSV)\n",
    "print(\"Saved:\", OUT_XLSX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
